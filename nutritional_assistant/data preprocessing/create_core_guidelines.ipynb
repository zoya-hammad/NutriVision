{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Dietary Guidelines Summarization\n",
    "\n",
    "This notebook extracts text from a file in the processed data folder, summarizes it using OpenAI's GPT-4, and saves the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file from processed data\n",
    "with open('processed_data/cleaned_dietary_guidelines.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks to handle long sequences\n",
    "chunk_size = 8000  \n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 96000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total characters: {len(chunks)*chunk_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You will be given a set of dietary guideline texts. Based on these, generate similar content that follows the same structure and tone, with an emphasis on practical, food- and recipe-based recommendations. Follow these instructions:\n",
    "\n",
    "Omit any explicit mention of the target population.\n",
    "\n",
    "Do not include research basis or references to levels of evidence.\n",
    "\n",
    "Focus on clear, actionable dietary recommendations with food examples (e.g., “Use olive oil instead of butter” or “Include legumes like lentils and chickpeas in stews”).\n",
    "\n",
    "Emphasize preparation methods, ingredient swaps, and daily food practices. Write in an instructional tone.\n",
    "\n",
    "Prioritize realistic advice, especially related to meals and cooking habits.\n",
    "\n",
    "Keep each guideline section concise and clear.\n",
    "\n",
    "Do not use overly technical terms or abstract nutritional concepts.\n",
    "\n",
    "You will receive the base text in sections. For each section, generate a rewritten version following the above rules\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(chunk):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt },\n",
    "            {\"role\": \"user\", \"content\": f\"Please summarize the following text, limiting your answer to 300 words with key points:\\n\\n{chunk}\"}\n",
    "        ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/12...\n",
      "Processing chunk 2/12...\n",
      "Processing chunk 3/12...\n",
      "Processing chunk 4/12...\n",
      "Processing chunk 5/12...\n",
      "Processing chunk 6/12...\n",
      "Processing chunk 7/12...\n",
      "Processing chunk 8/12...\n",
      "Processing chunk 9/12...\n",
      "Processing chunk 10/12...\n",
      "Processing chunk 11/12...\n",
      "Processing chunk 12/12...\n"
     ]
    }
   ],
   "source": [
    "# Process each chunk and collect summaries\n",
    "summaries = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages= messages_for(chunk),\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    summary = response.choices[0].message.content\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all summaries\n",
    "combined_summary = ' '.join(summaries)\n",
    "\n",
    "# Save the summary to processed data\n",
    "with open('processed_data/core_dietary_guidelines.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
