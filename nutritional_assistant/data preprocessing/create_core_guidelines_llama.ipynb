{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72516005",
   "metadata": {},
   "source": [
    "# Core Dietary Guidelines Summarization using Ollama\n",
    "This notebook extracts text from a file in the processed data folder, summarizes it using Ollama's Llama model, and saves the summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8926d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ollama\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Constants\n",
    "MODEL = \"llama3.2:1b\"  # Using the smaller 1B parameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52174956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL = \"llama3.2:1b\"  # Using the smaller 1B parameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7c94ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 74701a8c35f6... 100% ▕████████████████▏ 1.3 GB                         \n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \n",
      "pulling 4f659a1e86d7... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69db5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 96000\n"
     ]
    }
   ],
   "source": [
    "# Load the text file from processed data\n",
    "with open('processed_data/cleaned_dietary_guidelines.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Split text into chunks to handle long sequences\n",
    "chunk_size = 8000  \n",
    "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "print(f\"Total characters: {len(chunks)*chunk_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c61a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for the model\n",
    "system_prompt = \"\"\"You will be given a set of dietary guideline texts. Based on these, generate similar content that follows the same structure and tone, with an emphasis on practical, food- and recipe-based recommendations. Follow these instructions:\n",
    "Omit any explicit mention of the target population.\n",
    "Do not include research basis or references to levels of evidence.\n",
    "Focus on clear, actionable dietary recommendations with food examples (e.g., \"Use olive oil instead of butter\" or \"Include legumes like lentils and chickpeas in stews\").\n",
    "Emphasize preparation methods, ingredient swaps, and daily food practices. Write in an instructional tone.\n",
    "Prioritize realistic advice, especially related to meals and cooking habits.\n",
    "Keep each guideline section concise and clear.\n",
    "Do not use overly technical terms or abstract nutritional concepts.\n",
    "You will receive the base text in sections. For each section, generate a rewritten version following the above rules\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683e9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(chunk):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Please summarize the following text, limiting your answer to 300 words with key points:\\n\\n{chunk}\"}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddc826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/12...\n",
      "Processing chunk 2/12...\n",
      "Processing chunk 3/12...\n",
      "Processing chunk 4/12...\n",
      "Processing chunk 5/12...\n",
      "Processing chunk 6/12...\n",
      "Processing chunk 7/12...\n",
      "Processing chunk 8/12...\n",
      "Processing chunk 9/12...\n",
      "Processing chunk 10/12...\n",
      "Processing chunk 11/12...\n",
      "Processing chunk 12/12...\n"
     ]
    }
   ],
   "source": [
    "# Process each chunk and collect summaries\n",
    "summaries = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=messages_for(chunk)\n",
    "    )\n",
    "    \n",
    "    summary = response['message']['content']\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Combine all summaries\n",
    "combined_summary = ' '.join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03ed319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the summary to processed data\n",
    "with open('processed_data/core_dietary_guidelines_ollama.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adedcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
