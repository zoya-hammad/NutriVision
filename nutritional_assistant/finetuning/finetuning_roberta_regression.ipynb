{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning RoBERTa for Glycemic Load Regression\n",
    "\n",
    "This notebook fine-tunes a RoBERTa model for glycemic load regression using the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets wandb tqdm huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer,RobertaForSequenceClassification,RobertaConfig,AdamW,get_linear_schedule_with_warmup,pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access API keys from Colab secrets\n",
    "HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
    "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "wandb.init(\n",
    "    project='nutrivision',\n",
    "    name='roberta-regression',\n",
    "    config={\n",
    "        'model_name': 'roberta-base',\n",
    "        'task': 'glycemic_load_regression',\n",
    "        'max_length': 512,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-5,\n",
    "        'num_epochs': 3,\n",
    "        'warmup_steps': 0,\n",
    "        'weight_decay': 0.01,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'max_grad_norm': 1.0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset\n",
    "with open('roberta_regression_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f'Training set size: {len(train_df)}')\n",
    "print(f'Validation set size: {len(val_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and DataLoader Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlycemicLoadDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'target': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GlycemicLoadDataset(\n",
    "    train_df['text'].values,\n",
    "    train_df['glycemic_load'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = GlycemicLoadDataset(\n",
    "    val_df['text'].values,\n",
    "    val_df['glycemic_load'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size)\n",
    "\n",
    "# Initialize model\n",
    "config = RobertaConfig.from_pretrained('roberta-base', num_labels=1)  # 1 output for regression\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=config)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=wandb.config.learning_rate,\n",
    "    weight_decay=wandb.config.weight_decay\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * wandb.config.num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=wandb.config.warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(wandb.config.num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{wandb.config.num_epochs}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), wandb.config.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Log batch metrics\n",
    "        wandb.log({\n",
    "            'batch_loss': loss.item(),\n",
    "            'learning_rate': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = outputs.logits.squeeze()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_mse = mean_squared_error(all_targets, all_preds)\n",
    "    val_r2 = r2_score(all_targets, all_preds)\n",
    "    \n",
    "    # Log epoch metrics\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_mse': val_mse,\n",
    "        'val_r2': val_r2\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{wandb.config.num_epochs}:')\n",
    "    print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "    print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "    print(f'Validation MSE: {val_mse:.4f}')\n",
    "    print(f'Validation RÂ²: {val_r2:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_pretrained('models/roberta_regression')\n",
    "        tokenizer.save_pretrained('models/roberta_regression')\n",
    "        print('Saved best model!')\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new repository on Hugging Face Hub\n",
    "repo_name = \"nutrivision/roberta-glycemic-load-regression\"\n",
    "try:\n",
    "    create_repo(repo_name, token=HUGGINGFACE_TOKEN, repo_type=\"model\")\n",
    "    print(f\"Created new repository: {repo_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository might already exist: {e}\")\n",
    "\n",
    "# Load the best model\n",
    "model = RobertaForSequenceClassification.from_pretrained('models/roberta_regression')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('models/roberta_regression')\n",
    "\n",
    "# Push model and tokenizer to Hugging Face Hub\n",
    "model.push_to_hub(repo_name, token=HUGGINGFACE_TOKEN)\n",
    "tokenizer.push_to_hub(repo_name, token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "print(f\"Model and tokenizer uploaded to {repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Uploaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from Hugging Face Hub\n",
    "regression_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=repo_name,\n",
    "    tokenizer=repo_name,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test with a sample text\n",
    "sample_text = \"1 cup of cooked white rice\"\n",
    "prediction = regression_pipeline(sample_text)\n",
    "print(f\"Sample text: {sample_text}\")\n",
    "print(f\"Predicted glycemic load: {prediction[0]['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions vs actual plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_targets, all_preds, alpha=0.5)\n",
    "plt.plot([min(all_targets), max(all_targets)], [min(all_targets), max(all_targets)], 'r--')\n",
    "plt.xlabel('Actual Glycemic Load')\n",
    "plt.ylabel('Predicted Glycemic Load')\n",
    "plt.title('Predicted vs Actual Glycemic Load')\n",
    "plt.show()\n",
    "\n",
    "# Create residual plot\n",
    "residuals = np.array(all_preds) - np.array(all_targets)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_preds, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Glycemic Load')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
