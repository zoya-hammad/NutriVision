{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfqnMfMIDfP5"
   },
   "source": [
    "# Fine-tuning RoBERTa for 25-Class Classification\n",
    "\n",
    "This notebook fine-tunes a RoBERTa model to classify recipes into 25 classes based on their nutritional value (0-100). The classes are:\n",
    "- Class 0: 0-4\n",
    "- Class 1: 5-8\n",
    "- Class 2: 9-12\n",
    "- Class 3: 13-16\n",
    "- Class 4: 17-20\n",
    "- Class 5: 21-24\n",
    "- Class 6: 25-28\n",
    "- Class 7: 29-32\n",
    "- Class 8: 33-36\n",
    "- Class 9: 37-40\n",
    "- Class 10: 41-44\n",
    "- Class 11: 45-48\n",
    "- Class 12: 49-52\n",
    "- Class 13: 53-56\n",
    "- Class 14: 57-60\n",
    "- Class 15: 61-64\n",
    "- Class 16: 65-68\n",
    "- Class 17: 69-72\n",
    "- Class 18: 73-76\n",
    "- Class 19: 77-80\n",
    "- Class 20: 81-84\n",
    "- Class 21: 85-88\n",
    "- Class 22: 89-92\n",
    "- Class 23: 93-96\n",
    "- Class 24: 97-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLjbYzb-DfP7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcRbFsrcFwb7"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "f_rEyu04DfP8",
    "outputId": "98868c2c-14ed-4fa2-f1a5-861e7d27f4c4"
   },
   "outputs": [],
   "source": [
    "# Access API keys from Colab secrets\n",
    "HUGGINGFACE_TOKEN = userdata.get('HF_TOKEN')\n",
    "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"nutrivision-roberta-classification-25\",\n",
    "    config={\n",
    "        \"architecture\": \"RoBERTa\",\n",
    "        \"dataset\": \"recipe-classification-25\",\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_classes\": 25\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xttcC0mGDfP8"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for item in data:\n",
    "        texts.append(item['text'])\n",
    "        # Convert regression value to class (0-24)\n",
    "        label = int(item['label'] // 4)\n",
    "        if label > 24:  # Handle edge case where label is 100\n",
    "            label = 24\n",
    "        labels.append(label)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Load data\n",
    "texts, labels = load_data('roberta_regression_data.json')\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElES7s1KDfP8"
   },
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xttcC0mGDfP9"
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=25)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RecipeDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = RecipeDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xttcC0mGDfP10"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    wandb.log({\"train_loss\": avg_loss})\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"val_accuracy\": val_accuracy\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    print(f'Average Training Loss: {avg_loss:.4f}')\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xttcC0mGDfP11"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('nutrivision-roberta-25')\n",
    "tokenizer.save_pretrained('nutrivision-roberta-25')\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "model.push_to_hub('zoya-hammadk/nutrivision-roberta-25')\n",
    "tokenizer.push_to_hub('zoya-hammadk/nutrivision-roberta-25')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "finetuning_roberta_2_25classes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
