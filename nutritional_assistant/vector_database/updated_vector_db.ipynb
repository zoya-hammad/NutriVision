{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82beee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\92310\\anaconda3\\envs\\llms\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee14c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\92310\\anaconda3\\envs\\llms\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f185971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_recipe(doc: str, metadata: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a recipe document into structured format.\n",
    "    \n",
    "    Args:\n",
    "        doc (str): Raw recipe document\n",
    "        metadata (dict): Recipe metadata\n",
    "        \n",
    "    Returns:\n",
    "        dict: Parsed recipe with title, ingredients, instructions, etc.\n",
    "    \"\"\"\n",
    "    # Extract title\n",
    "    title_match = re.search(r'Title: (.*?)(?:\\n|$)', doc)\n",
    "    title = title_match.group(1) if title_match else metadata.get('title', '')\n",
    "    \n",
    "    # Extract ingredients\n",
    "    ingredients_match = re.search(r'Ingredients: (.*?)(?:\\n|$)', doc)\n",
    "    ingredients = ingredients_match.group(1).split(', ') if ingredients_match else []\n",
    "    \n",
    "    # Extract instructions\n",
    "    instructions_match = re.search(r'Instructions: (.*?)(?:\\n|$)', doc)\n",
    "    instructions = instructions_match.group(1) if instructions_match else ''\n",
    "    \n",
    "    # Extract NER\n",
    "    ner_match = re.search(r'NER: (.*?)(?:\\n|$)', doc)\n",
    "    ner = ner_match.group(1) if ner_match else ''\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'ingredients': ingredients,\n",
    "        'instructions': instructions,\n",
    "        'ner': ner,\n",
    "        'raw_text': doc,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "def create_vector_db(test_ratio: float = 0.2):\n",
    "    \"\"\"\n",
    "    Create vector database for training data and save test data as JSON.\n",
    "    \n",
    "    Args:\n",
    "        test_ratio (float): Ratio of data to use for testing (default: 0.2)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"ashikan/diabetic-friendly-recipes\")\n",
    "    recipes = dataset['train']\n",
    "    \n",
    "    # Split into train and test\n",
    "    test_size = int(len(recipes) * test_ratio)\n",
    "    train_recipes = recipes.select(range(test_size, len(recipes)))\n",
    "    test_recipes = recipes.select(range(test_size))\n",
    "    \n",
    "    # Initialize ChromaDB client for training data\n",
    "    DB_PATH = \"recipes_vectorstore\"\n",
    "    client = chromadb.PersistentClient(path=DB_PATH)\n",
    "    \n",
    "    # Create or get collection\n",
    "    collection_name = \"recipes\"\n",
    "    existing_collections = [collection.name for collection in client.list_collections()]\n",
    "    if collection_name in existing_collections:\n",
    "        client.delete_collection(collection_name)\n",
    "        print(f\"Deleted existing collection: {collection_name}\")\n",
    "    \n",
    "    collection = client.create_collection(collection_name)\n",
    "    \n",
    "    # Initialize the sentence transformer model\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Process training recipes for vector DB\n",
    "    print(\"Processing training recipes for vector database...\")\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    \n",
    "    for idx, recipe in enumerate(tqdm(train_recipes)):\n",
    "        # Create a text representation of the recipe\n",
    "        recipe_text = f\"Title: {recipe['recipeName']}\\n\"\n",
    "        recipe_text += f\"Ingredients: {', '.join(recipe['ingredients'])}\\n\"\n",
    "        recipe_text += f\"Instructions: {recipe['steps']}\\n\"\n",
    "        recipe_text += f\"NER: {recipe['NER']}\\n\"\n",
    "        \n",
    "        metadata = {\n",
    "            'title': recipe['recipeName'],\n",
    "            'ingredients_count': len(recipe['ingredients']),\n",
    "            'instructions_length': len(recipe['steps']),\n",
    "            'serves': recipe['serves']\n",
    "        }\n",
    "        \n",
    "        documents.append(recipe_text)\n",
    "        metadatas.append(metadata)\n",
    "        ids.append(str(idx))\n",
    "    \n",
    "    # Generate embeddings and add to collection\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(documents), batch_size)):\n",
    "        batch_docs = documents[i:i + batch_size]\n",
    "        batch_metadatas = metadatas[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = model.encode(batch_docs).tolist()\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            documents=batch_docs,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=batch_metadatas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    # Process test recipes for JSON\n",
    "    print(\"Processing test recipes for JSON...\")\n",
    "    test_parsed = []\n",
    "    \n",
    "    for recipe in tqdm(test_recipes):\n",
    "        recipe_text = f\"Title: {recipe['recipeName']}\\n\"\n",
    "        recipe_text += f\"Ingredients: {', '.join(recipe['ingredients'])}\\n\"\n",
    "        recipe_text += f\"Instructions: {recipe['steps']}\\n\"\n",
    "        recipe_text += f\"NER: {recipe['NER']}\\n\"\n",
    "        \n",
    "        metadata = {\n",
    "            'title': recipe['recipeName'],\n",
    "            'ingredients_count': len(recipe['ingredients']),\n",
    "            'instructions_length': len(recipe['steps']),\n",
    "            'serves': recipe['serves']\n",
    "        }\n",
    "        \n",
    "        parsed_recipe = parse_recipe(recipe_text, metadata)\n",
    "        test_parsed.append(parsed_recipe)\n",
    "    \n",
    "    # Save test recipes to JSON\n",
    "    with open('test_recipes.json', 'w') as f:\n",
    "        json.dump(test_parsed, f, indent=2)\n",
    "    \n",
    "    print(f\"Created vector database with {len(train_recipes)} training recipes\")\n",
    "    print(f\"Saved {len(test_recipes)} test recipes to test_recipes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d8f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: recipes\n",
      "Processing training recipes for vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 575/575 [00:00<00:00, 1420.71it/s]\n",
      "100%|██████████| 6/6 [01:01<00:00, 10.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test recipes for JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [00:00<00:00, 1748.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector database with 575 training recipes\n",
      "Saved 143 test recipes to test_recipes.json\n"
     ]
    }
   ],
   "source": [
    "create_vector_db() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
