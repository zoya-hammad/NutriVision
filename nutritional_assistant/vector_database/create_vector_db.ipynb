{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vector Database for Diabetic-Friendly Recipes\n",
    "\n",
    "This notebook creates a vector database from the diabetic-friendly recipes dataset using ChromaDB and sentence transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loads the “ashikan/diabetic-friendly-recipes” dataset from HuggingFace, ensuring access to a high-quality, real-world data source.\n",
    "- Processes and formats recipe data (title, ingredients, instructions, nutritional info) for semantic search and retrieval.\n",
    "- Generates dense vector embeddings for each recipe using a state-of-the-art SentenceTransformer model, enabling advanced AI-powered similarity search.\n",
    "- Creates a persistent ChromaDB vector database to store recipe embeddings and metadata, supporting scalable and efficient retrieval-augmented generation (RAG) workflows.\n",
    "- Saves all embeddings to disk for downstream visualization and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\92310\\anaconda3\\envs\\llms\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"ashikan/diabetic-friendly-recipes\")\n",
    "recipes = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "DB_PATH = \"recipes_vectorstore\"\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Create or get collection\n",
    "collection_name = \"recipes\"\n",
    "existing_collections = [collection.name for collection in client.list_collections()]\n",
    "if collection_name in existing_collections:\n",
    "    client.delete_collection(collection_name)\n",
    "    print(f\"Deleted existing collection: {collection_name}\")\n",
    "\n",
    "collection = client.create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits compared to OpenAI embeddings:\n",
    "\n",
    "It's free and fast, and we can run it locally, so the data never leaves our box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/718 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 718/718 [00:00<00:00, 3949.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for vector database\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for idx, recipe in enumerate(tqdm(recipes)):\n",
    "    # Create a text representation of the recipe\n",
    "    recipe_text = f\"Title: {recipe['recipeName']}\\n\"\n",
    "    recipe_text += f\"Ingredients: {', '.join(recipe['ingredients'])}\\n\"\n",
    "    recipe_text += f\"Instructions: {recipe['steps']}\\n\"\n",
    "    recipe_text += f\"NER: {recipe['NER']}\\n\"\n",
    "\n",
    "    documents.append(recipe_text)\n",
    "    metadatas.append({\n",
    "        'title': recipe['recipeName'],\n",
    "        'ingredients_count': len(recipe['ingredients']),\n",
    "        'instructions_length': len(recipe['steps'])\n",
    "    })\n",
    "    ids.append(str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:16<00:00,  9.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings and add to collection\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    batch_docs = documents[i:i + batch_size]\n",
    "    batch_metadatas = metadatas[i:i + batch_size]\n",
    "    batch_ids = ids[i:i + batch_size]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(batch_docs).tolist()\n",
    "    \n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=batch_docs,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=batch_metadatas,\n",
    "        ids=batch_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
