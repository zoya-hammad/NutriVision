{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GI Prediction Category Testing\n",
    "\n",
    "This notebook tests our GI prediction system across different recipe categories:\n",
    "1. Simple recipes\n",
    "2. High GI recipes\n",
    "3. Low GI recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from agents.gi_agent import GIAnalysisAgent\n",
    "from agents.gi_grader_agent import GIGraderAgent\n",
    "from test_data.simple.simple_recipes import SIMPLE_RECIPES\n",
    "from test_data.high_gi.high_gi_recipes import HIGH_GI_RECIPES\n",
    "from test_data.low_gi.low_gi_recipes import LOW_GI_RECIPES\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Results Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for this test run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_dir = f'test_results/{timestamp}'\n",
    "plots_dir = f'{results_dir}/plots'\n",
    "data_dir = f'{results_dir}/data'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved in: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "gi_agent = GIAnalysisAgent()\n",
    "grader_agent = GIGraderAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_category(recipes: List[Dict[str, Any]], category_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Test a category of recipes and return results\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for recipe in recipes:\n",
    "        # Get our prediction\n",
    "        our_result = gi_agent.process([recipe])\n",
    "        if 'error' in our_result:\n",
    "            print(f\"Error processing {recipe['title']}: {our_result['error']}\")\n",
    "            continue\n",
    "            \n",
    "        our_prediction = our_result['glycemic_load']\n",
    "        \n",
    "        # Get expert grading\n",
    "        grade_result = grader_agent.grade_prediction(recipe, our_prediction)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'recipe': recipe['title'],\n",
    "            'category': category_name,\n",
    "            'our_prediction': our_prediction,\n",
    "            'expert_assessment': grade_result['assessed_gi'],\n",
    "            'prediction_difference': grade_result['prediction_difference']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each category\n",
    "simple_results = test_category(SIMPLE_RECIPES, \"Simple\")\n",
    "high_gi_results = test_category(HIGH_GI_RECIPES, \"High GI\")\n",
    "low_gi_results = test_category(LOW_GI_RECIPES, \"Low GI\")\n",
    "\n",
    "# Combine results\n",
    "all_results = pd.concat([simple_results, high_gi_results, low_gi_results])\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Box plot of prediction differences by category\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='category', y='prediction_difference', data=all_results)\n",
    "plt.title('Prediction Differences by Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Absolute Difference')\n",
    "\n",
    "# Scatter plot of our predictions vs expert assessments\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.scatterplot(data=all_results, x='our_prediction', y='expert_assessment', hue='category')\n",
    "plt.title('Our Predictions vs Expert Assessments')\n",
    "plt.xlabel('Our Prediction')\n",
    "plt.ylabel('Expert Assessment')\n",
    "\n",
    "# Add diagonal line for perfect predictions\n",
    "max_val = max(all_results['our_prediction'].max(), all_results['expert_assessment'].max())\n",
    "plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.3)\n",
    "\n",
    "# Box plot of percentage differences by category\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(x='category', y='percentage_difference', data=all_results)\n",
    "plt.title('Percentage Differences by Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Percentage Difference (%)')\n",
    "\n",
    "# Distribution of prediction differences\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(data=all_results, x='prediction_difference', hue='category', multiple='stack')\n",
    "plt.title('Distribution of Prediction Differences')\n",
    "plt.xlabel('Prediction Difference')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plots_dir}/gi_analysis.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional metrics\n",
    "all_results['percentage_difference'] = (all_results['prediction_difference'] / all_results['expert_assessment']) * 100\n",
    "rmse = np.sqrt(np.mean(all_results['prediction_difference'] ** 2))\n",
    "\n",
    "print(\"\\nOverall Statistics:\")\n",
    "print(\"=================\")\n",
    "print(f\"Total recipes tested: {len(all_results)}\")\n",
    "print(f\"Mean prediction difference: {all_results['prediction_difference'].mean():.2f}\")\n",
    "print(f\"Mean percentage difference: {all_results['percentage_difference'].mean():.2f}%\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Standard deviation: {all_results['prediction_difference'].std():.2f}\")\n",
    "print(f\"Max difference: {all_results['prediction_difference'].max():.2f}\")\n",
    "print(f\"Min difference: {all_results['prediction_difference'].min():.2f}\")\n",
    "\n",
    "print(\"\\nCategory-wise Statistics:\")\n",
    "print(\"======================\")\n",
    "for category in ['Simple', 'High GI', 'Low GI']:\n",
    "    cat_results = all_results[all_results['category'] == category]\n",
    "    cat_rmse = np.sqrt(np.mean(cat_results['prediction_difference'] ** 2))\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"Number of recipes: {len(cat_results)}\")\n",
    "    print(f\"Mean difference: {cat_results['prediction_difference'].mean():.2f}\")\n",
    "    print(f\"Mean percentage difference: {cat_results['percentage_difference'].mean():.2f}%\")\n",
    "    print(f\"RMSE: {cat_rmse:.2f}\")\n",
    "    print(f\"Standard deviation: {cat_results['prediction_difference'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to CSV\n",
    "all_results.to_csv(f'{data_dir}/gi_test_results.csv', index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'overall': {\n",
    "        'total_recipes': len(all_results),\n",
    "        'mean_difference': all_results['prediction_difference'].mean(),\n",
    "        'mean_percentage_difference': all_results['percentage_difference'].mean(),\n",
    "        'rmse': rmse,\n",
    "        'std_difference': all_results['prediction_difference'].std(),\n",
    "        'max_difference': all_results['prediction_difference'].max(),\n",
    "        'min_difference': all_results['prediction_difference'].min()\n",
    "    },\n",
    "    'by_category': {\n",
    "        category: {\n",
    "            'count': len(all_results[all_results['category'] == category]),\n",
    "            'mean_difference': all_results[all_results['category'] == category]['prediction_difference'].mean(),\n",
    "            'mean_percentage_difference': all_results[all_results['category'] == category]['percentage_difference'].mean(),\n",
    "            'rmse': np.sqrt(np.mean(all_results[all_results['category'] == category]['prediction_difference'] ** 2)),\n",
    "            'std_difference': all_results[all_results['category'] == category]['prediction_difference'].std()\n",
    "        }\n",
    "        for category in ['Simple', 'High GI', 'Low GI']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{data_dir}/summary_stats.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=4)\n",
    "\n",
    "print(f\"\\nResults have been saved to {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}